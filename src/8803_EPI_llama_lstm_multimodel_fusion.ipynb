{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bfb05f7e4c144a1d8253400ee866b04e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0f2999b10a64b9d849c61071c24bd1d",
              "IPY_MODEL_9e5356de13a44e4bb6eb297276b6e490",
              "IPY_MODEL_9c10a41979614f0fa845deb1f643ba5e"
            ],
            "layout": "IPY_MODEL_c348a7d3926d49518f7ddd42a337381d"
          }
        },
        "e0f2999b10a64b9d849c61071c24bd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54f58b879f2d4131a9c7cfa47a28cc32",
            "placeholder": "​",
            "style": "IPY_MODEL_3ba2227a6f2047cb9ef1c2409f0cd595",
            "value": "Adding EOS to train dataset: 100%"
          }
        },
        "9e5356de13a44e4bb6eb297276b6e490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5099d4c8458a40cbadcb5fec7e3a2622",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_501972d32caa41ef9e3e6fe31a21c2c8",
            "value": 190
          }
        },
        "9c10a41979614f0fa845deb1f643ba5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb55fc0653814c33a8e419ef71b7fbeb",
            "placeholder": "​",
            "style": "IPY_MODEL_4247a8467e5d48b7a0e65e6f583cac4b",
            "value": " 190/190 [00:00&lt;00:00, 11051.57 examples/s]"
          }
        },
        "c348a7d3926d49518f7ddd42a337381d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f58b879f2d4131a9c7cfa47a28cc32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba2227a6f2047cb9ef1c2409f0cd595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5099d4c8458a40cbadcb5fec7e3a2622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501972d32caa41ef9e3e6fe31a21c2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb55fc0653814c33a8e419ef71b7fbeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4247a8467e5d48b7a0e65e6f583cac4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02c86cf95d504c828cceb0316f3d2e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3934bf9e108e4ccbb5fa4bb4eab45590",
              "IPY_MODEL_424165bd39e44c288db081a7dc0daa8e",
              "IPY_MODEL_ca3e2d074da84596978ddbd11855974b"
            ],
            "layout": "IPY_MODEL_2462490f72e041d69056ad637e4cad93"
          }
        },
        "3934bf9e108e4ccbb5fa4bb4eab45590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c277337bfd1a417e93d58e49dbccf37d",
            "placeholder": "​",
            "style": "IPY_MODEL_19e6e8b37efe4f98a050193eedd0e5b5",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "424165bd39e44c288db081a7dc0daa8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1e2d07c608741da83f868a693ce11f1",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcfdc3387c634ffdb66ea2567b0f714f",
            "value": 190
          }
        },
        "ca3e2d074da84596978ddbd11855974b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e0150155784007acbd8acd24ed9fa7",
            "placeholder": "​",
            "style": "IPY_MODEL_3fa57f362e0a49b0b16acd5a083780df",
            "value": " 190/190 [00:00&lt;00:00, 1506.19 examples/s]"
          }
        },
        "2462490f72e041d69056ad637e4cad93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c277337bfd1a417e93d58e49dbccf37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e6e8b37efe4f98a050193eedd0e5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1e2d07c608741da83f868a693ce11f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcfdc3387c634ffdb66ea2567b0f714f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85e0150155784007acbd8acd24ed9fa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fa57f362e0a49b0b16acd5a083780df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef707c8fe175492bbc3edf3b4690ba62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbcf81f7ff1645ca807a2cb267788112",
              "IPY_MODEL_76a11547f46040a199c37ec96be2b1e4",
              "IPY_MODEL_81da7c0093a146abb6f8e56023692840"
            ],
            "layout": "IPY_MODEL_82d279d677414442bf10edb507f49ea0"
          }
        },
        "dbcf81f7ff1645ca807a2cb267788112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2963c1639dd84772ba13186ced1afacb",
            "placeholder": "​",
            "style": "IPY_MODEL_fee412635ab64db69f3ecd0f9db37396",
            "value": "Truncating train dataset: 100%"
          }
        },
        "76a11547f46040a199c37ec96be2b1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cafbc6c0dc342b8ac15c49e3a0908ad",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_199acf76070a47239b9fbf36d0369875",
            "value": 190
          }
        },
        "81da7c0093a146abb6f8e56023692840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab2815b2f4d0414494d7510fc42b9640",
            "placeholder": "​",
            "style": "IPY_MODEL_dbb3f1df0cdd42a691aefa5285ae4f8c",
            "value": " 190/190 [00:00&lt;00:00, 16243.41 examples/s]"
          }
        },
        "82d279d677414442bf10edb507f49ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2963c1639dd84772ba13186ced1afacb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee412635ab64db69f3ecd0f9db37396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cafbc6c0dc342b8ac15c49e3a0908ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "199acf76070a47239b9fbf36d0369875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab2815b2f4d0414494d7510fc42b9640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbb3f1df0cdd42a691aefa5285ae4f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "813773631aee4268b2a84f2779027215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d0a7db36a6c404caf5f5a6e725f59b2",
              "IPY_MODEL_d6429d3272474a2a818724df10303915",
              "IPY_MODEL_45b09a17444843e091d79a63c7b377a6"
            ],
            "layout": "IPY_MODEL_9b5dd39b5ddb4ea9a969fd2b25003d0a"
          }
        },
        "1d0a7db36a6c404caf5f5a6e725f59b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf21fc71a8543b7a5f1b8141cdd47b2",
            "placeholder": "​",
            "style": "IPY_MODEL_aabfe371542e4b108fa1ea35a3e5e364",
            "value": "Adding EOS to train dataset: 100%"
          }
        },
        "d6429d3272474a2a818724df10303915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e338661aab94ec194051ddfa804fff4",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f10bf1efd14a43ec8d79a65257ac5ee5",
            "value": 570
          }
        },
        "45b09a17444843e091d79a63c7b377a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47a98d93e0a648e89fd654207d5e7a81",
            "placeholder": "​",
            "style": "IPY_MODEL_f8de0563cd624e64b048f08e8fc2d9f0",
            "value": " 570/570 [00:00&lt;00:00, 19581.73 examples/s]"
          }
        },
        "9b5dd39b5ddb4ea9a969fd2b25003d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf21fc71a8543b7a5f1b8141cdd47b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aabfe371542e4b108fa1ea35a3e5e364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e338661aab94ec194051ddfa804fff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10bf1efd14a43ec8d79a65257ac5ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47a98d93e0a648e89fd654207d5e7a81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8de0563cd624e64b048f08e8fc2d9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdc4d8e9f3e54e6684451201d9fd2f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0756b8daf6d74efa8a34e8ed56c5dfb0",
              "IPY_MODEL_43fee8309f9b407baa62cd16b586b0b4",
              "IPY_MODEL_006bbd7ee95042cc9ba4db5106dc6f52"
            ],
            "layout": "IPY_MODEL_b1ca65c9977b4adb9b11aa23f9220663"
          }
        },
        "0756b8daf6d74efa8a34e8ed56c5dfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3285a0dca1fd42328807a5196693c0af",
            "placeholder": "​",
            "style": "IPY_MODEL_c3f050dc4acb420dbb643a749b346e83",
            "value": "Tokenizing train dataset: 100%"
          }
        },
        "43fee8309f9b407baa62cd16b586b0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d1da5144d4e4d8d9a2882a627984f9d",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d98bc7157244a16949a4d762f15d7ab",
            "value": 570
          }
        },
        "006bbd7ee95042cc9ba4db5106dc6f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ca4dc714b7a481c9da7776f5a818cdb",
            "placeholder": "​",
            "style": "IPY_MODEL_e08a87400c134df285be7e81ad940009",
            "value": " 570/570 [00:00&lt;00:00, 1982.72 examples/s]"
          }
        },
        "b1ca65c9977b4adb9b11aa23f9220663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3285a0dca1fd42328807a5196693c0af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3f050dc4acb420dbb643a749b346e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d1da5144d4e4d8d9a2882a627984f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d98bc7157244a16949a4d762f15d7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ca4dc714b7a481c9da7776f5a818cdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e08a87400c134df285be7e81ad940009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d67b80a68f34824a82d339dbddfd56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4274f5ed71cc485e8af7736fa9d95008",
              "IPY_MODEL_d83228294c6e4d24838556de35dac56d",
              "IPY_MODEL_b225cc77f2944d5993eae1088eaade3d"
            ],
            "layout": "IPY_MODEL_82c41dc3aeec4828a61845c27d7c29c7"
          }
        },
        "4274f5ed71cc485e8af7736fa9d95008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d09f7e219740444b80fee6caa0a862bd",
            "placeholder": "​",
            "style": "IPY_MODEL_81029db8489047a38362bf476e61cae7",
            "value": "Truncating train dataset: 100%"
          }
        },
        "d83228294c6e4d24838556de35dac56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0740d93c0dc74bdd90255c66c31370bb",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55493bdf47b046b4bbd66fb7eb72a82a",
            "value": 570
          }
        },
        "b225cc77f2944d5993eae1088eaade3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_282338a1924b44e7a40062ddd3f4c465",
            "placeholder": "​",
            "style": "IPY_MODEL_3fa6e5689570495f805ff703816ee19c",
            "value": " 570/570 [00:00&lt;00:00, 52915.01 examples/s]"
          }
        },
        "82c41dc3aeec4828a61845c27d7c29c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d09f7e219740444b80fee6caa0a862bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81029db8489047a38362bf476e61cae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0740d93c0dc74bdd90255c66c31370bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55493bdf47b046b4bbd66fb7eb72a82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "282338a1924b44e7a40062ddd3f4c465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fa6e5689570495f805ff703816ee19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb6c6d8797f643ca9d633c5492dd41ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_83ad9797f1e84233acd2e2173f64b804"
          }
        },
        "51d48525e782442cb64dfc76782f8c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11f73b76ad754d4795cc19bca810b72b",
            "placeholder": "​",
            "style": "IPY_MODEL_cfbeac58d27142c4bc345bb97ef12351",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d2e98e86f3834a01830d3ff388da4c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c6a82391efb94d079788beebcac2f921",
            "placeholder": "​",
            "style": "IPY_MODEL_44bd74ee99d548f6b460cf5798613577",
            "value": ""
          }
        },
        "ac3a97770b204314a61b2d875040d5e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_c7658bfab6334894bb8eda070f58d69a",
            "style": "IPY_MODEL_15a46f4a278e438f989c0100e5a1be46",
            "value": true
          }
        },
        "01b652c8aaa545729e1ed5acf607e92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4dbf04d21a2147eda5dbc5223e38a22a",
            "style": "IPY_MODEL_e18dec39e32248259226f57b81ba0b9d",
            "tooltip": ""
          }
        },
        "7830243a465941f8a85d0f813e619fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d59d9930ef04d0692ac08cab9cd0aff",
            "placeholder": "​",
            "style": "IPY_MODEL_40b98edd1e444d5885a953c4225a64a5",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "83ad9797f1e84233acd2e2173f64b804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "11f73b76ad754d4795cc19bca810b72b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfbeac58d27142c4bc345bb97ef12351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a82391efb94d079788beebcac2f921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44bd74ee99d548f6b460cf5798613577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7658bfab6334894bb8eda070f58d69a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a46f4a278e438f989c0100e5a1be46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dbf04d21a2147eda5dbc5223e38a22a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e18dec39e32248259226f57b81ba0b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5d59d9930ef04d0692ac08cab9cd0aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40b98edd1e444d5885a953c4225a64a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6380000a1b264bdeac05fa83fa0970e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_726ab7e3d24b414697fcd56c0a178355",
            "placeholder": "​",
            "style": "IPY_MODEL_c9bfbf9d8c5c47faa44ae0bf0636da1c",
            "value": "Connecting..."
          }
        },
        "726ab7e3d24b414697fcd56c0a178355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9bfbf9d8c5c47faa44ae0bf0636da1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fejIcThaf_fk",
        "outputId": "729f8382-b302-4baa-d853-3e5cba4d6b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/amazon-science/chronos-forecasting.git\n",
        "!pip install pandas torch transformers accelerate\n",
        "!pip install -U \"transformers>=4.40.0\" accelerate bitsandbytes\n",
        "!pip install -U transformers trl peft accelerate bitsandbytes datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WFynsJfLgEbb",
        "outputId": "915404df-69f6-4078-bf44-756d33a0b12a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/amazon-science/chronos-forecasting.git\n",
            "  Cloning https://github.com/amazon-science/chronos-forecasting.git to /tmp/pip-req-build-r0jzrtmf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/amazon-science/chronos-forecasting.git /tmp/pip-req-build-r0jzrtmf\n",
            "  Resolved https://github.com/amazon-science/chronos-forecasting.git to commit 9afe64332f2456188da9375daa57e87eff7512ca\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate<2,>=0.34 in /usr/local/lib/python3.12/dist-packages (from chronos-forecasting==2.2.0rc3) (1.12.0)\n",
            "Requirement already satisfied: einops<1,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from chronos-forecasting==2.2.0rc3) (0.8.1)\n",
            "Requirement already satisfied: numpy<3,>=1.21 in /usr/local/lib/python3.12/dist-packages (from chronos-forecasting==2.2.0rc3) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from chronos-forecasting==2.2.0rc3) (1.6.1)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from chronos-forecasting==2.2.0rc3) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers<5,>=4.41 in /usr/local/lib/python3.12/dist-packages (from chronos-forecasting==2.2.0rc3) (4.57.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=0.34->chronos-forecasting==2.2.0rc3) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=0.34->chronos-forecasting==2.2.0rc3) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=0.34->chronos-forecasting==2.2.0rc3) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=0.34->chronos-forecasting==2.2.0rc3) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=0.34->chronos-forecasting==2.2.0rc3) (0.7.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2,>=1.6.0->chronos-forecasting==2.2.0rc3) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2,>=1.6.0->chronos-forecasting==2.2.0rc3) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2,>=1.6.0->chronos-forecasting==2.2.0rc3) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<2,>=0.34->chronos-forecasting==2.2.0rc3) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->chronos-forecasting==2.2.0rc3) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5,>=4.41->chronos-forecasting==2.2.0rc3) (2025.11.12)\n",
            "Building wheels for collected packages: chronos-forecasting\n",
            "  Building wheel for chronos-forecasting (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chronos-forecasting: filename=chronos_forecasting-2.2.0rc3-py3-none-any.whl size=72076 sha256=2bd9e61a62f5288f54d1c1a874327bfa4b5e4e961be5dd9a73829f5decc4c93f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k7ib_fgp/wheels/b9/a6/b5/75fca7306751a3bc92a63680f861f44a42a8776f6423cf0188\n",
            "Successfully built chronos-forecasting\n",
            "Installing collected packages: chronos-forecasting\n",
            "Successfully installed chronos-forecasting-2.2.0rc3\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Collecting transformers>=4.40.0\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.40.0) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.40.0) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, bitsandbytes\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "Successfully installed bitsandbytes-0.48.2 transformers-4.57.3\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Collecting trl\n",
            "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Downloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, datasets, trl\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed datasets-4.4.1 pyarrow-22.0.0 trl-0.25.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              },
              "id": "295ec6d812574b3382759edd6d105ba2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "eb6c6d8797f643ca9d633c5492dd41ee",
            "51d48525e782442cb64dfc76782f8c07",
            "d2e98e86f3834a01830d3ff388da4c75",
            "ac3a97770b204314a61b2d875040d5e9",
            "01b652c8aaa545729e1ed5acf607e92c",
            "7830243a465941f8a85d0f813e619fb6",
            "83ad9797f1e84233acd2e2173f64b804",
            "11f73b76ad754d4795cc19bca810b72b",
            "cfbeac58d27142c4bc345bb97ef12351",
            "c6a82391efb94d079788beebcac2f921",
            "44bd74ee99d548f6b460cf5798613577",
            "c7658bfab6334894bb8eda070f58d69a",
            "15a46f4a278e438f989c0100e5a1be46",
            "4dbf04d21a2147eda5dbc5223e38a22a",
            "e18dec39e32248259226f57b81ba0b9d",
            "5d59d9930ef04d0692ac08cab9cd0aff",
            "40b98edd1e444d5885a953c4225a64a5",
            "6380000a1b264bdeac05fa83fa0970e1",
            "726ab7e3d24b414697fcd56c0a178355",
            "c9bfbf9d8c5c47faa44ae0bf0636da1c"
          ]
        },
        "id": "LPr1JvhygHwN",
        "outputId": "eeb5dca9-5f7b-4a31-ab3d-9680c2f7d616"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb6c6d8797f643ca9d633c5492dd41ee"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from chronos import ChronosPipeline"
      ],
      "metadata": {
        "id": "aWlMiPK6CR-8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timeseries_path = \"/content/drive/MyDrive/covid_us_timeseries.csv\"\n",
        "sentiment_path = \"/content/drive/MyDrive/covid_ts_sentiment_combined.csv\"\n",
        "\n",
        "\n",
        "timeseries_df = pd.read_csv(timeseries_path)\n",
        "sentiment_df = pd.read_csv(sentiment_path)\n",
        "\n",
        "data = pd.merge(timeseries_df, sentiment_df , on=\"date\", how=\"inner\")"
      ],
      "metadata": {
        "id": "HRdQVdaVCCTk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = data.copy()\n",
        "data_clean = data_clean.ffill().bfill()\n",
        "\n",
        "# Ensure date is the index\n",
        "data_clean.set_index('date', inplace=True)\n",
        "df = data_clean.copy()"
      ],
      "metadata": {
        "id": "rj8mKCxECWmu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "warnings.simplefilter('ignore', UserWarning)\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "# We compare Daily Cases, Daily Deaths, and Cumulative Vaccinations\n",
        "targets = ['new_cases', 'new_deaths', 'people_vaccinated']\n",
        "senti_col = 'fear_intensity'\n",
        "TAU = 14\n",
        "\n",
        "print(f\"--- Running SARIMAX Comparison for {len(targets)} Targets ---\")\n",
        "\n",
        "# ==========================================\n",
        "# MAIN LOOP\n",
        "# ==========================================\n",
        "for target_col in targets:\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"🎯 TARGET: {target_col}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. DATA PREP & SHIFTING\n",
        "    df_exp = df.copy()\n",
        "\n",
        "    # Check if column exists\n",
        "    if target_col not in df_exp.columns:\n",
        "        print(f\"⚠️ Column '{target_col}' not found. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Create the Shifted Feature\n",
        "    df_exp['fear_shifted'] = df_exp[senti_col].shift(TAU)\n",
        "\n",
        "    # Drop NaNs (Critical for alignment)\n",
        "    df_exp = df_exp.dropna(subset=[target_col, senti_col, 'fear_shifted'])\n",
        "\n",
        "    # 2. SCALING (0-1 Range for Friend Comparison)\n",
        "    scaler_y = MinMaxScaler()\n",
        "    scaler_x = MinMaxScaler()\n",
        "\n",
        "    y_raw = df_exp[target_col].values.reshape(-1, 1)\n",
        "    y_scaled = scaler_y.fit_transform(y_raw).flatten()\n",
        "\n",
        "    x_raw_vals = df_exp[senti_col].values.reshape(-1, 1)\n",
        "    x_shift_vals = df_exp['fear_shifted'].values.reshape(-1, 1)\n",
        "\n",
        "    x_raw_scaled = scaler_x.fit_transform(x_raw_vals)\n",
        "    x_shift_scaled = scaler_x.fit_transform(x_shift_vals)\n",
        "\n",
        "    # Split (80/20)\n",
        "    split = int(len(y_scaled) * 0.8)\n",
        "\n",
        "    y_train, y_test = y_scaled[:split], y_scaled[split:]\n",
        "    x_raw_train, x_raw_test = x_raw_scaled[:split], x_raw_scaled[split:]\n",
        "    x_shift_train, x_shift_test = x_shift_scaled[:split], x_shift_scaled[split:]\n",
        "\n",
        "    # Also keep an unscaled version of y_test for nicer plots\n",
        "    y_test_orig = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 3. RUN CONFIGURATIONS\n",
        "    configs = [\n",
        "        (\"1. Baseline (No Sentiment)\", None, None),\n",
        "        (\"2. With Sentiment (Raw)\", x_raw_train, x_raw_test),\n",
        "        (f\"3. With Sentiment (Tau={TAU})\", x_shift_train, x_shift_test)\n",
        "    ]\n",
        "\n",
        "    print(f\"{'Configuration':<35} | {'Test MSE':<10} | {'Improvement'}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    baseline_mse = 0\n",
        "\n",
        "    for i, (name, train_exog, test_exog) in enumerate(configs):\n",
        "        try:\n",
        "            # Initialize SARIMAX\n",
        "            model = SARIMAX(\n",
        "                y_train,\n",
        "                exog=train_exog,\n",
        "                order=(14, 0, 0),\n",
        "                seasonal_order=(0, 0, 0, 0),\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False\n",
        "            )\n",
        "\n",
        "            model_fit = model.fit(disp=False)\n",
        "            forecast_res = model_fit.get_forecast(steps=len(y_test), exog=test_exog)\n",
        "            y_pred = forecast_res.predicted_mean\n",
        "\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "            # Improvement calculation\n",
        "            if i == 0:\n",
        "                baseline_mse = mse\n",
        "                imp_str = \"-\"\n",
        "            else:\n",
        "                diff = baseline_mse - mse\n",
        "                pct = (diff / baseline_mse) * 100\n",
        "                if pct > 0:\n",
        "                    imp_str = f\"✅ +{pct:.1f}%\"\n",
        "                else:\n",
        "                    imp_str = f\"❌ {pct:.1f}%\"\n",
        "\n",
        "            print(f\"{name:<35} | {mse:.5f}    | {imp_str}\")\n",
        "\n",
        "            # ==========================\n",
        "            # PLOT PRED vs TRUTH & SAVE\n",
        "            # ==========================\n",
        "            # Convert prediction back to original scale\n",
        "            y_pred_orig = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            plt.plot(y_test_orig, label='Truth', linewidth=2)\n",
        "            plt.plot(y_pred_orig, label='Prediction', linestyle='--')\n",
        "            plt.title(f\"{target_col} – {name}\")\n",
        "            plt.xlabel(\"Time (test index)\")\n",
        "            plt.ylabel(target_col)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Safe filename, e.g. \"new_cases_config1.png\"\n",
        "            fname = f\"{target_col}_config{i+1}.png\".replace(\" \", \"_\")\n",
        "            plt.savefig(fname, dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "            # Optional: print where it was saved\n",
        "            print(f\"   📊 Saved plot to: {fname}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{name:<35} | FAILED       | {str(e)[:50]}...\")\n",
        "\n",
        "print(\"-\" * 65)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpOyfH4uiF9V",
        "outputId": "bd128b78-d321-4364-f761-0cb8f0fef37c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running SARIMAX Comparison for 3 Targets ---\n",
            "\n",
            "============================================================\n",
            "🎯 TARGET: new_cases\n",
            "============================================================\n",
            "Configuration                       | Test MSE   | Improvement\n",
            "-----------------------------------------------------------------\n",
            "1. Baseline (No Sentiment)          | 0.17947    | -\n",
            "   📊 Saved plot to: new_cases_config1.png\n",
            "2. With Sentiment (Raw)             | 0.16730    | ✅ +6.8%\n",
            "   📊 Saved plot to: new_cases_config2.png\n",
            "3. With Sentiment (Tau=14)          | 0.15832    | ✅ +11.8%\n",
            "   📊 Saved plot to: new_cases_config3.png\n",
            "\n",
            "============================================================\n",
            "🎯 TARGET: new_deaths\n",
            "============================================================\n",
            "Configuration                       | Test MSE   | Improvement\n",
            "-----------------------------------------------------------------\n",
            "1. Baseline (No Sentiment)          | 0.05084    | -\n",
            "   📊 Saved plot to: new_deaths_config1.png\n",
            "2. With Sentiment (Raw)             | 0.04921    | ✅ +3.2%\n",
            "   📊 Saved plot to: new_deaths_config2.png\n",
            "3. With Sentiment (Tau=14)          | 0.04450    | ✅ +12.5%\n",
            "   📊 Saved plot to: new_deaths_config3.png\n",
            "\n",
            "============================================================\n",
            "🎯 TARGET: people_vaccinated\n",
            "============================================================\n",
            "Configuration                       | Test MSE   | Improvement\n",
            "-----------------------------------------------------------------\n",
            "1. Baseline (No Sentiment)          | 0.00313    | -\n",
            "   📊 Saved plot to: people_vaccinated_config1.png\n",
            "2. With Sentiment (Raw)             | 0.00030    | ✅ +90.3%\n",
            "   📊 Saved plot to: people_vaccinated_config2.png\n",
            "3. With Sentiment (Tau=14)          | 0.00043    | ✅ +86.1%\n",
            "   📊 Saved plot to: people_vaccinated_config3.png\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt   # <-- added for plotting\n",
        "\n",
        "# ==========================================\n",
        "# 1. DEFINE FUSION MODEL (Dual-Encoder)\n",
        "# ==========================================\n",
        "class DualBranchFusionLSTM(nn.Module):\n",
        "    def __init__(self, senti_dim, hidden_dim=32):\n",
        "        super(DualBranchFusionLSTM, self).__init__()\n",
        "\n",
        "        # Branch 1: Temporal Encoder (Target History)\n",
        "        self.lstm_target = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "        # Branch 2: Sentiment Encoder (Sentiment History)\n",
        "        self.lstm_senti = nn.LSTM(input_size=senti_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "        # Fusion Layer (Combines the two branches)\n",
        "        self.fusion_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 16),  # Fuse 32+32 -> 16\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1)  # Predict scalar\n",
        "        )\n",
        "\n",
        "    def forward(self, x_target, x_senti):\n",
        "        # Pass through separate LSTMs\n",
        "        _, (h_target, _) = self.lstm_target(x_target)\n",
        "        _, (h_senti, _) = self.lstm_senti(x_senti)\n",
        "\n",
        "        # Concatenate the final hidden states\n",
        "        # h shape is (1, batch, hidden) -> remove dim 0\n",
        "        feat_target = h_target[-1]\n",
        "        feat_senti = h_senti[-1]\n",
        "\n",
        "        combined = torch.cat((feat_target, feat_senti), dim=1)\n",
        "\n",
        "        # Predict\n",
        "        return self.fusion_head(combined)\n",
        "\n",
        "# ==========================================\n",
        "# 2. CONFIGURATION\n",
        "# ==========================================\n",
        "targets = ['new_cases', 'new_deaths', 'people_vaccinated']\n",
        "senti_cols = ['fear_intensity', 'valence_intensity', 'anger_intensity',\n",
        "              'happiness_intensity', 'sadness_intensity']\n",
        "TAU = 14\n",
        "\n",
        "print(f\"--- Running Multimodal Fusion LSTM on {len(targets)} Targets ---\")\n",
        "print(f\"{'Target':<20} | {'Scaled MSE':<12} | {'Real MAE':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN LOOP\n",
        "# ==========================================\n",
        "for target_col in targets:\n",
        "    if target_col not in df.columns:\n",
        "        continue\n",
        "\n",
        "    # --- A. Data Prep ---\n",
        "    df_exp = df.copy()\n",
        "    # Apply Shift to Sentiment\n",
        "    for c in senti_cols:\n",
        "        df_exp[c] = df_exp[c].shift(TAU)\n",
        "    df_exp = df_exp.dropna()\n",
        "\n",
        "    # Scale Data (0-1)\n",
        "    scaler_target = MinMaxScaler()\n",
        "    scaler_senti = MinMaxScaler()\n",
        "\n",
        "    y_raw = df_exp[target_col].values.reshape(-1, 1)\n",
        "    x_senti_raw = df_exp[senti_cols].values\n",
        "\n",
        "    y_scaled = scaler_target.fit_transform(y_raw)\n",
        "    x_senti_scaled = scaler_senti.fit_transform(x_senti_raw)\n",
        "\n",
        "    # Create Sequences (Dual Input)\n",
        "    X_target, X_senti, y = [], [], []\n",
        "    lookback = 30\n",
        "\n",
        "    for i in range(len(y_scaled) - lookback):\n",
        "        # Input 1: History of Target\n",
        "        X_target.append(y_scaled[i:i+lookback])\n",
        "        # Input 2: History of Sentiment\n",
        "        X_senti.append(x_senti_scaled[i:i+lookback])\n",
        "        # Output: Next Target\n",
        "        y.append(y_scaled[i+lookback])\n",
        "\n",
        "    X_target = torch.tensor(np.array(X_target), dtype=torch.float32).to(device)\n",
        "    X_senti = torch.tensor(np.array(X_senti), dtype=torch.float32).to(device)\n",
        "    y = torch.tensor(np.array(y), dtype=torch.float32).to(device)\n",
        "\n",
        "    # Split\n",
        "    split = int(len(y) * 0.8)\n",
        "\n",
        "    # Datasets\n",
        "    train_ds = TensorDataset(X_target[:split], X_senti[:split], y[:split])\n",
        "    test_ds = TensorDataset(X_target[split:], X_senti[split:], y[split:])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "    # --- B. Train ---\n",
        "    model = DualBranchFusionLSTM(senti_dim=len(senti_cols)).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(50):\n",
        "        for b_t, b_s, b_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(b_t, b_s).squeeze()        # (batch,)\n",
        "            loss = criterion(preds, b_y.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # --- C. Evaluate ---\n",
        "    model.eval()\n",
        "    preds_scaled = []\n",
        "    actuals_scaled = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for b_t, b_s, b_y in test_loader:\n",
        "            preds = model(b_t, b_s).squeeze()        # (batch,)\n",
        "            preds_scaled.extend(preds.cpu().numpy().ravel())\n",
        "            actuals_scaled.extend(b_y.cpu().numpy().ravel())\n",
        "\n",
        "    preds_scaled = np.array(preds_scaled).reshape(-1, 1)\n",
        "    actuals_scaled = np.array(actuals_scaled).reshape(-1, 1)\n",
        "\n",
        "    # Calculate Metrics\n",
        "    mse_scaled = mean_squared_error(actuals_scaled, preds_scaled)\n",
        "\n",
        "    # Inverse Transform for Real MAE\n",
        "    preds_real = scaler_target.inverse_transform(preds_scaled)\n",
        "    actuals_real = scaler_target.inverse_transform(actuals_scaled)\n",
        "    mae_real = mean_absolute_error(actuals_real, preds_real)\n",
        "\n",
        "    print(f\"{target_col:<20} | {mse_scaled:.5f}      | {mae_real:,.0f}\")\n",
        "\n",
        "    # ==========================================\n",
        "    # D. PLOT PRED vs TRUTH (NORMALIZED) & SAVE\n",
        "    # ==========================================\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    # use the scaled values in [0, 1]\n",
        "    plt.plot(actuals_scaled.flatten(), label=\"Truth\", linewidth=2)\n",
        "    plt.plot(preds_scaled.flatten(), label=\"Prediction\", linestyle=\"--\")\n",
        "\n",
        "    plt.title(f\"Fusion LSTM – {target_col} (normalized)\")\n",
        "    plt.xlabel(\"Date\")  # or \"Time (test index)\" if you prefer\n",
        "    plt.ylabel(\"Cases (normalized)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fname = f\"fusion_lstm_{target_col}_normalized.png\"\n",
        "    plt.savefig(fname, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"   📊 Saved plot to: {fname}\")\n",
        "\n",
        "print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PYVxmvwjq9f",
        "outputId": "439acb77-dcd5-4003-9a1f-3132fd88a179"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Multimodal Fusion LSTM on 3 Targets ---\n",
            "Target               | Scaled MSE   | Real MAE  \n",
            "--------------------------------------------------\n",
            "new_cases            | 0.01357      | 24,059\n",
            "   📊 Saved plot to: fusion_lstm_new_cases_normalized.png\n",
            "new_deaths           | 0.01899      | 374\n",
            "   📊 Saved plot to: fusion_lstm_new_deaths_normalized.png\n",
            "people_vaccinated    | 0.00023      | 2,334,748\n",
            "   📊 Saved plot to: fusion_lstm_people_vaccinated_normalized.png\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ============================================================\n",
        "# 1. Check model/tokenizer availability\n",
        "# ============================================================\n",
        "try:\n",
        "    model\n",
        "    tokenizer\n",
        "except:\n",
        "    raise RuntimeError(\"❌ 'model' and 'tokenizer' must already be loaded in memory!\")\n",
        "\n",
        "if 'df' not in globals():\n",
        "    raise RuntimeError(\"❌ 'df' not found. Load your timeseries first!\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Forecaster in LLMTime format\n",
        "# ============================================================\n",
        "def get_llama_prediction(history_list, debug=False):\n",
        "    \"\"\"Returns 1-step prediction using existing Llama model.\"\"\"\n",
        "\n",
        "    history_str = \", \".join([str(int(x)) for x in history_list])\n",
        "\n",
        "    prompt = (\n",
        "        f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        f\"You are a pattern completion engine. Given the sequence of numbers, \"\n",
        "        f\"predict the next ONE number. Return only the number. No text.<|eot_id|>\"\n",
        "        f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        f\"Sequence: {history_str}\\n\"\n",
        "        f\"Next number:<|eot_id|>\"\n",
        "        f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=10,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if debug:\n",
        "        print(\"============== RAW MODEL RESPONSE ==============\")\n",
        "        print(response)\n",
        "        print(\"================================================\")\n",
        "\n",
        "    # Try parse number from assistant reply\n",
        "    try:\n",
        "        if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
        "            raw_answer = response.split(\"<|start_header_id|>assistant<|end_header_id|>\", 1)[-1]\n",
        "        else:\n",
        "            raw_answer = response\n",
        "\n",
        "        nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", raw_answer)\n",
        "        if nums:\n",
        "            return float(nums[-1])\n",
        "\n",
        "        # fallback: persistence\n",
        "        return float(history_list[-1])\n",
        "    except:\n",
        "        return float(history_list[-1])\n",
        "\n",
        "# ============================================================\n",
        "# 3. Prepare data\n",
        "# ============================================================\n",
        "TARGET = \"new_cases\"\n",
        "\n",
        "raw_series = df[TARGET].values.astype(float)\n",
        "split = int(len(raw_series) * 0.8)\n",
        "\n",
        "test_series = raw_series[split:]\n",
        "history_window = 20\n",
        "\n",
        "eval_len = min(30, len(test_series) - history_window)\n",
        "indices = range(len(test_series) - eval_len, len(test_series))\n",
        "\n",
        "actuals, preds = [], []\n",
        "\n",
        "# ============================================================\n",
        "# 4. Fit scaler for Scaled MSE\n",
        "# ============================================================\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(raw_series.reshape(-1, 1))\n",
        "\n",
        "# ============================================================\n",
        "# 5. Run evaluation (first 3 samples printed raw)\n",
        "# ============================================================\n",
        "print(f\"Evaluating last {len(indices)} test points...\\n\")\n",
        "\n",
        "for k, i in enumerate(indices, start=1):\n",
        "    hist = test_series[i - history_window : i]\n",
        "    actual = test_series[i]\n",
        "\n",
        "    pred = get_llama_prediction(hist, debug=(k <= 3))\n",
        "\n",
        "    actuals.append(actual)\n",
        "    preds.append(pred)\n",
        "\n",
        "    print(f\"[{k}] True={int(actual)}, Pred={int(pred)}\")\n",
        "\n",
        "actuals = np.array(actuals, dtype=float)\n",
        "preds = np.array(preds, dtype=float)\n",
        "\n",
        "# ============================================================\n",
        "# 6. Compute MAE & Scaled MSE\n",
        "# ============================================================\n",
        "mae = mean_absolute_error(actuals, preds)\n",
        "\n",
        "scaled_actuals = scaler.transform(actuals.reshape(-1, 1))\n",
        "scaled_preds = scaler.transform(preds.reshape(-1, 1))\n",
        "scaled_mse = mean_squared_error(scaled_actuals, scaled_preds)\n",
        "\n",
        "# ============================================================\n",
        "# 7. Print results\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 Base Llama-3 8B (No Finetuning)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"MAE:        {mae:,.2f}\")\n",
        "print(f\"Scaled MSE: {scaled_mse:.6f}\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF4tl5uI5BAI",
        "outputId": "5c09b77e-53fd-43c7-fe21-75cf0648a7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating last 30 test points...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given the sequence of numbers, predict the next ONE number. Return only the number. No text.user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150\n",
            "Next number:assistant\n",
            "\n",
            "106151\n",
            "================================================\n",
            "[1] True=106150, Pred=106151\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given the sequence of numbers, predict the next ONE number. Return only the number. No text.user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150, 106150\n",
            "Next number:assistant\n",
            "\n",
            "106150\n",
            "================================================\n",
            "[2] True=106150, Pred=106150\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given the sequence of numbers, predict the next ONE number. Return only the number. No text.user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150, 106150, 106150\n",
            "Next number:assistant\n",
            "\n",
            "106150\n",
            "================================================\n",
            "[3] True=106150, Pred=106150\n",
            "[4] True=106150, Pred=106150\n",
            "[5] True=106150, Pred=106150\n",
            "[6] True=106150, Pred=106150\n",
            "[7] True=126637, Pred=106150\n",
            "[8] True=126637, Pred=126637\n",
            "[9] True=126637, Pred=126637\n",
            "[10] True=126637, Pred=126637\n",
            "[11] True=126637, Pred=126637\n",
            "[12] True=126637, Pred=126637\n",
            "[13] True=126637, Pred=126637\n",
            "[14] True=145709, Pred=126637\n",
            "[15] True=145709, Pred=145710\n",
            "[16] True=145709, Pred=145710\n",
            "[17] True=145709, Pred=145710\n",
            "[18] True=145709, Pred=145710\n",
            "[19] True=145709, Pred=156766\n",
            "[20] True=145709, Pred=156776\n",
            "[21] True=156412, Pred=156776\n",
            "[22] True=156412, Pred=156412\n",
            "[23] True=156412, Pred=156412\n",
            "[24] True=156412, Pred=156412\n",
            "[25] True=156412, Pred=156412\n",
            "[26] True=156412, Pred=156413\n",
            "[27] True=156412, Pred=156413\n",
            "[28] True=163464, Pred=156413\n",
            "[29] True=163464, Pred=163465\n",
            "[30] True=163464, Pred=163465\n",
            "\n",
            "============================================================\n",
            "📊 Base Llama-3 8B (No Finetuning)\n",
            "============================================================\n",
            "MAE:        2,303.79\n",
            "Scaled MSE: 0.000700\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ============================================================\n",
        "# 0. Check model/tokenizer availability\n",
        "# ============================================================\n",
        "try:\n",
        "    model\n",
        "    tokenizer\n",
        "except NameError:\n",
        "    raise RuntimeError(\"❌ 'model' and 'tokenizer' must already be loaded in memory!\")\n",
        "\n",
        "if 'df' not in globals():\n",
        "    raise RuntimeError(\"❌ 'df' not found. Load your timeseries first!\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. Autocast helper (fix Float vs Half issue)\n",
        "# ============================================================\n",
        "def autocast_ctx():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32)\n",
        "    return nullcontext()\n",
        "\n",
        "# ============================================================\n",
        "# 2. Forecaster with <answer> tags + \"closest number\" heuristic\n",
        "# ============================================================\n",
        "def get_llama_prediction(history_list, debug=False):\n",
        "    \"\"\"\n",
        "    Returns 1-step prediction using existing Llama model.\n",
        "    - Asks model to put the number inside <answer>...</answer>.\n",
        "    - If multiple numbers appear, picks the one closest to the last history value.\n",
        "    \"\"\"\n",
        "\n",
        "    history_str = \", \".join([str(int(x)) for x in history_list])\n",
        "    last_val = float(history_list[-1])\n",
        "\n",
        "    prompt = (\n",
        "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        \"You are a pattern completion engine. Given a sequence of numbers, \"\n",
        "        \"predict the next ONE number.\\n\"\n",
        "        \"- Output ONLY the number, wrapped in <answer> and </answer>.\\n\"\n",
        "        \"- Do not add any extra text.\\n\\n\"\n",
        "        \"Example:\\n\"\n",
        "        \"Sequence: 10, 20, 30\\n\"\n",
        "        \"Next number:\\n\"\n",
        "        \"<answer>40</answer>\\n\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        f\"Sequence: {history_str}\\n\"\n",
        "        \"Next number:\\n\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with autocast_ctx():\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=16,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if debug:\n",
        "        print(\"============== RAW MODEL RESPONSE ==============\")\n",
        "        print(response)\n",
        "        print(\"================================================\")\n",
        "\n",
        "    try:\n",
        "        # 1) Prefer content inside <answer>...</answer>\n",
        "        if \"<answer>\" in response and \"</answer>\" in response:\n",
        "            raw_answer = response.split(\"<answer>\", 1)[-1].split(\"</answer>\", 1)[0]\n",
        "        else:\n",
        "            # 2) Otherwise, strip to the assistant chunk\n",
        "            if \"<|start_header_id|>assistant<|end_header_id|>\" in response:\n",
        "                raw_answer = response.split(\n",
        "                    \"<|start_header_id|>assistant<|end_header_id|>\", 1\n",
        "                )[-1]\n",
        "            else:\n",
        "                raw_answer = response\n",
        "\n",
        "        # 3) Find all numbers, then pick the one closest to last history value\n",
        "        num_strs = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", raw_answer)\n",
        "        if num_strs:\n",
        "            nums = [float(s) for s in num_strs]\n",
        "            best = min(nums, key=lambda v: abs(v - last_val))\n",
        "            return best\n",
        "\n",
        "        # 4) Fallback: persistence\n",
        "        return last_val\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Parsing error:\", e)\n",
        "        return last_val\n",
        "\n",
        "# ============================================================\n",
        "# 3. Evaluate a single target column\n",
        "# ============================================================\n",
        "def evaluate_target_base(target_col, history_window=20, eval_last=30):\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Column '{target_col}' not found in df\")\n",
        "\n",
        "    raw_series = df[target_col].values.astype(float)\n",
        "    split = int(len(raw_series) * 0.8)\n",
        "    test_series = raw_series[split:]\n",
        "\n",
        "    if len(test_series) <= history_window + 5:\n",
        "        raise ValueError(f\"Not enough test data for {target_col}\")\n",
        "\n",
        "    eval_len = min(eval_last, len(test_series) - history_window)\n",
        "    indices = range(len(test_series) - eval_len, len(test_series))\n",
        "\n",
        "    actuals, preds = [], []\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(raw_series.reshape(-1, 1))\n",
        "\n",
        "    print(f\"\\n🔍 Evaluating Base Model on '{target_col}' (last {len(indices)} pts)...\\n\")\n",
        "\n",
        "    for k, i in enumerate(indices, start=1):\n",
        "        hist = test_series[i - history_window : i]\n",
        "        actual = test_series[i]\n",
        "\n",
        "        pred = get_llama_prediction(hist, debug=(k <= 2))\n",
        "\n",
        "        actuals.append(actual)\n",
        "        preds.append(pred)\n",
        "\n",
        "        print(f\"[{k:02d}] {target_col:18s} | True={int(actual):10d} | Pred={int(pred):10d}\")\n",
        "\n",
        "    actuals = np.array(actuals, dtype=float)\n",
        "    preds = np.array(preds, dtype=float)\n",
        "\n",
        "    mae = mean_absolute_error(actuals, preds)\n",
        "\n",
        "    scaled_actuals = scaler.transform(actuals.reshape(-1, 1))\n",
        "    scaled_preds = scaler.transform(preds.reshape(-1, 1))\n",
        "    scaled_mse = mean_squared_error(scaled_actuals, scaled_preds)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"📊 Base Llama-3 — {target_col}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"MAE:        {mae:,.2f}\")\n",
        "    print(f\"Scaled MSE: {scaled_mse:.6f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return mae, scaled_mse\n",
        "\n",
        "# ============================================================\n",
        "# 4. Loop over all three targets\n",
        "# ============================================================\n",
        "TARGETS = ['new_cases', 'new_deaths', 'people_vaccinated']\n",
        "results = {}\n",
        "\n",
        "for col in TARGETS:\n",
        "    mae, mse = evaluate_target_base(col)\n",
        "    results[col] = (mae, mse)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Summary\n",
        "# ============================================================\n",
        "print(\"\\n===== 📌 SUMMARY — Base Llama-3 (All Targets) =====\")\n",
        "for col in TARGETS:\n",
        "    mae, mse = results[col]\n",
        "    print(f\"{col:20s} | MAE={mae:,.2f} | Scaled MSE={mse:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYlz5GrfFQP9",
        "outputId": "05e5888b-a779-4891-fed4-a3639a5c5b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Evaluating Base Model on 'new_cases' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given a sequence of numbers, predict the next ONE number.\n",
            "- Output ONLY the number, wrapped in <answer> and </answer>.\n",
            "- Do not add any extra text.\n",
            "\n",
            "Example:\n",
            "Sequence: 10, 20, 30\n",
            "Next number:\n",
            "<answer>40</answer>\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "<ICYICYICYICYICYICYICYICYICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[01] new_cases          | True=    106150 | Pred=    106150\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given a sequence of numbers, predict the next ONE number.\n",
            "- Output ONLY the number, wrapped in <answer> and </answer>.\n",
            "- Do not add any extra text.\n",
            "\n",
            "Example:\n",
            "Sequence: 10, 20, 30\n",
            "Next number:\n",
            "<answer>40</answer>\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "<ICYICYICYICYICYICYICYICYICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[02] new_cases          | True=    106150 | Pred=    106150\n",
            "[03] new_cases          | True=    106150 | Pred=    106150\n",
            "[04] new_cases          | True=    106150 | Pred=    106150\n",
            "[05] new_cases          | True=    106150 | Pred=    106150\n",
            "[06] new_cases          | True=    106150 | Pred=    106150\n",
            "[07] new_cases          | True=    126637 | Pred=    106150\n",
            "[08] new_cases          | True=    126637 | Pred=    126637\n",
            "[09] new_cases          | True=    126637 | Pred=    126637\n",
            "[10] new_cases          | True=    126637 | Pred=    126637\n",
            "[11] new_cases          | True=    126637 | Pred=    126637\n",
            "[12] new_cases          | True=    126637 | Pred=    126637\n",
            "[13] new_cases          | True=    126637 | Pred=    126637\n",
            "[14] new_cases          | True=    145709 | Pred=    126637\n",
            "[15] new_cases          | True=    145709 | Pred=    145709\n",
            "[16] new_cases          | True=    145709 | Pred=    145709\n",
            "[17] new_cases          | True=    145709 | Pred=    145709\n",
            "[18] new_cases          | True=    145709 | Pred=    145709\n",
            "[19] new_cases          | True=    145709 | Pred=    145709\n",
            "[20] new_cases          | True=    145709 | Pred=    145709\n",
            "[21] new_cases          | True=    156412 | Pred=    145709\n",
            "[22] new_cases          | True=    156412 | Pred=    156412\n",
            "[23] new_cases          | True=    156412 | Pred=    156412\n",
            "[24] new_cases          | True=    156412 | Pred=    156412\n",
            "[25] new_cases          | True=    156412 | Pred=    156412\n",
            "[26] new_cases          | True=    156412 | Pred=    156412\n",
            "[27] new_cases          | True=    156412 | Pred=    156412\n",
            "[28] new_cases          | True=    163464 | Pred=    156412\n",
            "[29] new_cases          | True=    163464 | Pred=    163464\n",
            "[30] new_cases          | True=    163464 | Pred=    163464\n",
            "\n",
            "============================================================\n",
            "📊 Base Llama-3 — new_cases\n",
            "============================================================\n",
            "MAE:        1,910.45\n",
            "Scaled MSE: 0.000615\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating Base Model on 'new_deaths' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given a sequence of numbers, predict the next ONE number.\n",
            "- Output ONLY the number, wrapped in <answer> and </answer>.\n",
            "- Do not add any extra text.\n",
            "\n",
            "Example:\n",
            "Sequence: 10, 20, 30\n",
            "Next number:\n",
            "<answer>40</answer>\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "<ICYICYICYICYICYICYICYICYICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[01] new_deaths         | True=       636 | Pred=       636\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given a sequence of numbers, predict the next ONE number.\n",
            "- Output ONLY the number, wrapped in <answer> and </answer>.\n",
            "- Do not add any extra text.\n",
            "\n",
            "Example:\n",
            "Sequence: 10, 20, 30\n",
            "Next number:\n",
            "<answer>40</answer>\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "<ICYICYICYICYICYICYICYICYICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[02] new_deaths         | True=       636 | Pred=       636\n",
            "[03] new_deaths         | True=       636 | Pred=       636\n",
            "[04] new_deaths         | True=       636 | Pred=       636\n",
            "[05] new_deaths         | True=       636 | Pred=       636\n",
            "[06] new_deaths         | True=       636 | Pred=       636\n",
            "[07] new_deaths         | True=       805 | Pred=       636\n",
            "[08] new_deaths         | True=       805 | Pred=       805\n",
            "[09] new_deaths         | True=       805 | Pred=       805\n",
            "[10] new_deaths         | True=       805 | Pred=       805\n",
            "[11] new_deaths         | True=       805 | Pred=       805\n",
            "[12] new_deaths         | True=       805 | Pred=       805\n",
            "[13] new_deaths         | True=       805 | Pred=       805\n",
            "[14] new_deaths         | True=      1150 | Pred=       805\n",
            "[15] new_deaths         | True=      1150 | Pred=      1150\n",
            "[16] new_deaths         | True=      1150 | Pred=      1150\n",
            "[17] new_deaths         | True=      1150 | Pred=      1150\n",
            "[18] new_deaths         | True=      1150 | Pred=      1150\n",
            "[19] new_deaths         | True=      1150 | Pred=      1150\n",
            "[20] new_deaths         | True=      1150 | Pred=      1150\n",
            "[21] new_deaths         | True=      1454 | Pred=      1150\n",
            "[22] new_deaths         | True=      1454 | Pred=      1454\n",
            "[23] new_deaths         | True=      1454 | Pred=      1454\n",
            "[24] new_deaths         | True=      1454 | Pred=      1454\n",
            "[25] new_deaths         | True=      1454 | Pred=      1454\n",
            "[26] new_deaths         | True=      1454 | Pred=      1454\n",
            "[27] new_deaths         | True=      1454 | Pred=      1454\n",
            "[28] new_deaths         | True=      1672 | Pred=      1454\n",
            "[29] new_deaths         | True=      1672 | Pred=      1672\n",
            "[30] new_deaths         | True=      1672 | Pred=      1672\n",
            "\n",
            "============================================================\n",
            "📊 Base Llama-3 — new_deaths\n",
            "============================================================\n",
            "MAE:        34.55\n",
            "Scaled MSE: 0.000989\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating Base Model on 'people_vaccinated' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given a sequence of numbers, predict the next ONE number.\n",
            "- Output ONLY the number, wrapped in <answer> and </answer>.\n",
            "- Do not add any extra text.\n",
            "\n",
            "Example:\n",
            "Sequence: 10, 20, 30\n",
            "Next number:\n",
            "<answer>40</answer>\n",
            "user\n",
            "\n",
            "Sequence: 186894730, 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "<ICYICYICYICYICYICYICYICYICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[01] people_vaccinated  | True= 194530994 | Pred= 193990270\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a pattern completion engine. Given a sequence of numbers, predict the next ONE number.\n",
            "- Output ONLY the number, wrapped in <answer> and </answer>.\n",
            "- Do not add any extra text.\n",
            "\n",
            "Example:\n",
            "Sequence: 10, 20, 30\n",
            "Next number:\n",
            "<answer>40</answer>\n",
            "user\n",
            "\n",
            "Sequence: 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270, 194530994\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "<ICYICYICYICYICYICYICYICYICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[02] people_vaccinated  | True= 195086109 | Pred= 194530994\n",
            "[03] people_vaccinated  | True= 195649555 | Pred= 195086109\n",
            "[04] people_vaccinated  | True= 196279987 | Pred= 195649555\n",
            "[05] people_vaccinated  | True= 196665066 | Pred= 196279987\n",
            "[06] people_vaccinated  | True= 196904648 | Pred= 196665066\n",
            "[07] people_vaccinated  | True= 197428551 | Pred= 196904648\n",
            "[08] people_vaccinated  | True= 197962296 | Pred= 197428551\n",
            "[09] people_vaccinated  | True= 198464661 | Pred= 197962296\n",
            "[10] people_vaccinated  | True= 198982786 | Pred= 198464661\n",
            "[11] people_vaccinated  | True= 199567586 | Pred= 198982786\n",
            "[12] people_vaccinated  | True= 199932043 | Pred= 199567586\n",
            "[13] people_vaccinated  | True= 200144925 | Pred= 199932043\n",
            "[14] people_vaccinated  | True= 200611812 | Pred= 200144925\n",
            "[15] people_vaccinated  | True= 201104856 | Pred= 200611812\n",
            "[16] people_vaccinated  | True= 201594173 | Pred= 201104856\n",
            "[17] people_vaccinated  | True= 202076701 | Pred= 201594173\n",
            "[18] people_vaccinated  | True= 202617121 | Pred= 202076701\n",
            "[19] people_vaccinated  | True= 202940961 | Pred= 202617121\n",
            "[20] people_vaccinated  | True= 203127143 | Pred= 202940961\n",
            "[21] people_vaccinated  | True= 203569005 | Pred= 203127143\n",
            "[22] people_vaccinated  | True= 204031035 | Pred= 203569005\n",
            "[23] people_vaccinated  | True= 204500481 | Pred= 204031035\n",
            "[24] people_vaccinated  | True= 204976451 | Pred= 204500481\n",
            "[25] people_vaccinated  | True= 205509915 | Pred= 204976451\n",
            "[26] people_vaccinated  | True= 205829068 | Pred= 205509915\n",
            "[27] people_vaccinated  | True= 206022846 | Pred= 205829068\n",
            "[28] people_vaccinated  | True= 206442182 | Pred= 206022846\n",
            "[29] people_vaccinated  | True= 206865219 | Pred= 206442182\n",
            "[30] people_vaccinated  | True= 207292449 | Pred= 206865219\n",
            "\n",
            "============================================================\n",
            "📊 Base Llama-3 — people_vaccinated\n",
            "============================================================\n",
            "MAE:        443,405.97\n",
            "Scaled MSE: 0.000005\n",
            "============================================================\n",
            "\n",
            "===== 📌 SUMMARY — Base Llama-3 (All Targets) =====\n",
            "new_cases            | MAE=1,910.45 | Scaled MSE=0.000615\n",
            "new_deaths           | MAE=34.55 | Scaled MSE=0.000989\n",
            "people_vaccinated    | MAE=443,405.97 | Scaled MSE=0.000005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# ---------- sanity ----------\n",
        "assert 'df' in globals(), \"df must be loaded\"\n",
        "assert 'model' in globals() and 'tokenizer' in globals(), \"model + tokenizer must be loaded\"\n",
        "\n",
        "TARGET = \"new_cases\"\n",
        "assert TARGET in df.columns, f\"{TARGET} must be in df\"\n",
        "\n",
        "# autocast helper (for generation later)\n",
        "def autocast_ctx():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32)\n",
        "    return nullcontext()\n",
        "\n",
        "# ---------- build QLoRA train dataset with strict 6-digit format ----------\n",
        "print(\"🔧 Building QLoRA dataset with 6-digit targets...\")\n",
        "\n",
        "raw_series = df[TARGET].values.astype(float)\n",
        "split_idx = int(len(raw_series) * 0.8)\n",
        "train_series = raw_series[:split_idx]\n",
        "\n",
        "history_window = 20\n",
        "train_samples = []\n",
        "\n",
        "FEW_SHOT = \"\"\"You are a pattern completion engine. Given a sequence of 6-digit integers, predict the next ONE 6-digit integer.\n",
        "Always output:\n",
        "- Exactly 6 digits\n",
        "- No commas, no spaces, no explanation\n",
        "- Wrapped in <answer> and </answer>\n",
        "\n",
        "Examples:\n",
        "Sequence: 031797, 031797, 031797\n",
        "Next: <answer>051289</answer>\n",
        "\n",
        "Sequence: 051289, 051289, 051289\n",
        "Next: <answer>078155</answer>\n",
        "\"\"\"\n",
        "\n",
        "for i in range(history_window, len(train_series)):\n",
        "    hist = train_series[i - history_window:i]\n",
        "    target_val = int(train_series[i])\n",
        "\n",
        "    # zero-pad to 6 digits\n",
        "    hist_str = \", \".join(f\"{int(x):06d}\" for x in hist)\n",
        "    target_str = f\"{target_val:06d}\"\n",
        "\n",
        "    text = (\n",
        "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        + FEW_SHOT\n",
        "        + \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        + f\"Sequence: {hist_str}\\n\"\n",
        "          \"Next:<|eot_id|>\"\n",
        "          \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        + f\"<answer>{target_str}</answer>\"\n",
        "    )\n",
        "\n",
        "    train_samples.append({\"text\": text})\n",
        "\n",
        "train_ds = Dataset.from_list(train_samples)\n",
        "print(f\"✅ QLoRA train samples: {len(train_ds)}\")\n",
        "\n",
        "# ---------- QLoRA config ----------\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./llama_new_cases_6digit_qlora\",\n",
        "    dataset_text_field=\"text\",\n",
        "    max_steps=60,                     # short run, but more than 40\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    peft_config=peft_config,\n",
        "    args=sft_config,\n",
        ")\n",
        "\n",
        "print(\"🚀 Starting QLoRA finetune (6-digit)...\")\n",
        "trainer.train()\n",
        "print(\"✅ QLoRA finetune (6-digit) done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "bfb05f7e4c144a1d8253400ee866b04e",
            "e0f2999b10a64b9d849c61071c24bd1d",
            "9e5356de13a44e4bb6eb297276b6e490",
            "9c10a41979614f0fa845deb1f643ba5e",
            "c348a7d3926d49518f7ddd42a337381d",
            "54f58b879f2d4131a9c7cfa47a28cc32",
            "3ba2227a6f2047cb9ef1c2409f0cd595",
            "5099d4c8458a40cbadcb5fec7e3a2622",
            "501972d32caa41ef9e3e6fe31a21c2c8",
            "eb55fc0653814c33a8e419ef71b7fbeb",
            "4247a8467e5d48b7a0e65e6f583cac4b",
            "02c86cf95d504c828cceb0316f3d2e6d",
            "3934bf9e108e4ccbb5fa4bb4eab45590",
            "424165bd39e44c288db081a7dc0daa8e",
            "ca3e2d074da84596978ddbd11855974b",
            "2462490f72e041d69056ad637e4cad93",
            "c277337bfd1a417e93d58e49dbccf37d",
            "19e6e8b37efe4f98a050193eedd0e5b5",
            "e1e2d07c608741da83f868a693ce11f1",
            "fcfdc3387c634ffdb66ea2567b0f714f",
            "85e0150155784007acbd8acd24ed9fa7",
            "3fa57f362e0a49b0b16acd5a083780df",
            "ef707c8fe175492bbc3edf3b4690ba62",
            "dbcf81f7ff1645ca807a2cb267788112",
            "76a11547f46040a199c37ec96be2b1e4",
            "81da7c0093a146abb6f8e56023692840",
            "82d279d677414442bf10edb507f49ea0",
            "2963c1639dd84772ba13186ced1afacb",
            "fee412635ab64db69f3ecd0f9db37396",
            "9cafbc6c0dc342b8ac15c49e3a0908ad",
            "199acf76070a47239b9fbf36d0369875",
            "ab2815b2f4d0414494d7510fc42b9640",
            "dbb3f1df0cdd42a691aefa5285ae4f8c"
          ]
        },
        "id": "fujE_ia49oG1",
        "outputId": "13979d0e-2501-4ea7-95ff-66b0d800ea2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Building QLoRA dataset with 6-digit targets...\n",
            "✅ QLoRA train samples: 190\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/190 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfb05f7e4c144a1d8253400ee866b04e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/190 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02c86cf95d504c828cceb0316f3d2e6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/190 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef707c8fe175492bbc3edf3b4690ba62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting QLoRA finetune (6-digit)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:24, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.845400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.923000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.422200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.342700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.298800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.264800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QLoRA finetune (6-digit) done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ============================================================\n",
        "# 0. Checks\n",
        "# ============================================================\n",
        "try:\n",
        "    model\n",
        "    tokenizer\n",
        "except NameError:\n",
        "    raise RuntimeError(\"'model' and 'tokenizer' must already be loaded\")\n",
        "\n",
        "if 'df' not in globals():\n",
        "    raise RuntimeError(\"'df' not found. Load your dataframe first\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. Autocast helper (fix Float vs Half)\n",
        "# ============================================================\n",
        "def autocast_ctx():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32)\n",
        "    return nullcontext()\n",
        "\n",
        "# ============================================================\n",
        "# 2. Dumb-simple predictor: prompt -> first number\n",
        "# ============================================================\n",
        "def get_llama_prediction(history_list, debug=False):\n",
        "    \"\"\"\n",
        "    Prompt Llama with a sequence of numbers and return the FIRST number\n",
        "    it generates after 'Next number:'.\n",
        "    \"\"\"\n",
        "    history_str = \", \".join(str(int(x)) for x in history_list)\n",
        "\n",
        "    prompt = (\n",
        "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        \"You are a model that predicts the next value in a numerical time series.\\n\"\n",
        "        \"Given a sequence of numbers, output ONLY the next number.\\n\"\n",
        "        \"No words, no explanation, just the number.\\n\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        f\"Sequence: {history_str}\\n\"\n",
        "        \"Next number:\\n\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with autocast_ctx():\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=8,          # small, we only need one number\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if debug:\n",
        "        print(\"============== RAW MODEL RESPONSE ==============\")\n",
        "        print(response)\n",
        "        print(\"================================================\")\n",
        "\n",
        "    # Just grab the FIRST number we see\n",
        "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", response)\n",
        "    if m:\n",
        "        return float(m.group(0))\n",
        "\n",
        "    # If model somehow gives no digits at all, fall back to last value\n",
        "    return float(history_list[-1])\n",
        "\n",
        "# ============================================================\n",
        "# 3. Evaluate a single target\n",
        "# ============================================================\n",
        "def evaluate_target_simple(target_col, history_window=20, eval_last=30):\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Column '{target_col}' not in df\")\n",
        "\n",
        "    raw_series = df[target_col].values.astype(float)\n",
        "    split = int(len(raw_series) * 0.8)\n",
        "    test_series = raw_series[split:]\n",
        "\n",
        "    if len(test_series) <= history_window + 5:\n",
        "        raise ValueError(f\"Not enough test data for {target_col}\")\n",
        "\n",
        "    eval_len = min(eval_last, len(test_series) - history_window)\n",
        "    indices = range(len(test_series) - eval_len, len(test_series))\n",
        "\n",
        "    actuals, preds = [], []\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(raw_series.reshape(-1, 1))\n",
        "\n",
        "    print(f\"\\n🔍 Evaluating Base Model on '{target_col}' (last {len(indices)} pts)...\\n\")\n",
        "\n",
        "    for k, i in enumerate(indices, start=1):\n",
        "        hist = test_series[i - history_window : i]\n",
        "        actual = test_series[i]\n",
        "\n",
        "        pred = get_llama_prediction(hist, debug=(k <= 2))\n",
        "\n",
        "        actuals.append(actual)\n",
        "        preds.append(pred)\n",
        "\n",
        "        print(f\"[{k:02d}] {target_col:18s} | True={int(actual):10d} | Pred={int(pred):10d}\")\n",
        "\n",
        "    actuals = np.array(actuals, dtype=float)\n",
        "    preds = np.array(preds, dtype=float)\n",
        "\n",
        "    mae = mean_absolute_error(actuals, preds)\n",
        "\n",
        "    scaled_actuals = scaler.transform(actuals.reshape(-1, 1))\n",
        "    scaled_preds = scaler.transform(preds.reshape(-1, 1))\n",
        "    scaled_mse = mean_squared_error(scaled_actuals, scaled_preds)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"📊 Base Llama-3 — {target_col}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"MAE:        {mae:,.2f}\")\n",
        "    print(f\"Scaled MSE: {scaled_mse:.6f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return mae, scaled_mse\n",
        "\n",
        "# ============================================================\n",
        "# 4. Run for all three targets\n",
        "# ============================================================\n",
        "TARGETS = ['new_cases', 'new_deaths', 'people_vaccinated']\n",
        "results = {}\n",
        "\n",
        "for col in TARGETS:\n",
        "    mae, mse = evaluate_target_simple(col)\n",
        "    results[col] = (mae, mse)\n",
        "\n",
        "print(\"\\n===== SUMMARY — Base Llama-3 (Simple numeric output) =====\")\n",
        "for col in TARGETS:\n",
        "    mae, mse = results[col]\n",
        "    print(f\"{col:20s} | MAE={mae:,.2f} | Scaled MSE={mse:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBif5EMvDPMw",
        "outputId": "8e583cee-9ea3-44d0-ed38-3c84e59b61b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Evaluating Base Model on 'new_cases' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "106ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[01] new_cases          | True=    106150 | Pred=     31797\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "106ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[02] new_cases          | True=    106150 | Pred=     31797\n",
            "[03] new_cases          | True=    106150 | Pred=     31797\n",
            "[04] new_cases          | True=    106150 | Pred=     31797\n",
            "[05] new_cases          | True=    106150 | Pred=     31797\n",
            "[06] new_cases          | True=    106150 | Pred=     51289\n",
            "[07] new_cases          | True=    126637 | Pred=     51289\n",
            "[08] new_cases          | True=    126637 | Pred=     51289\n",
            "[09] new_cases          | True=    126637 | Pred=     51289\n",
            "[10] new_cases          | True=    126637 | Pred=     51289\n",
            "[11] new_cases          | True=    126637 | Pred=     51289\n",
            "[12] new_cases          | True=    126637 | Pred=     51289\n",
            "[13] new_cases          | True=    126637 | Pred=     78155\n",
            "[14] new_cases          | True=    145709 | Pred=     78155\n",
            "[15] new_cases          | True=    145709 | Pred=     78155\n",
            "[16] new_cases          | True=    145709 | Pred=     78155\n",
            "[17] new_cases          | True=    145709 | Pred=     78155\n",
            "[18] new_cases          | True=    145709 | Pred=     78155\n",
            "[19] new_cases          | True=    145709 | Pred=     78155\n",
            "[20] new_cases          | True=    145709 | Pred=    106150\n",
            "[21] new_cases          | True=    156412 | Pred=    106150\n",
            "[22] new_cases          | True=    156412 | Pred=    106150\n",
            "[23] new_cases          | True=    156412 | Pred=    106150\n",
            "[24] new_cases          | True=    156412 | Pred=    106150\n",
            "[25] new_cases          | True=    156412 | Pred=    106150\n",
            "[26] new_cases          | True=    156412 | Pred=    106150\n",
            "[27] new_cases          | True=    156412 | Pred=    126637\n",
            "[28] new_cases          | True=    163464 | Pred=    126637\n",
            "[29] new_cases          | True=    163464 | Pred=    126637\n",
            "[30] new_cases          | True=    163464 | Pred=    126637\n",
            "\n",
            "============================================================\n",
            "📊 Base Llama-3 — new_cases\n",
            "============================================================\n",
            "MAE:        60,464.25\n",
            "Scaled MSE: 0.075356\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating Base Model on 'new_deaths' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "636InsensitiveBKInsensitiveBKInsensitiveBKICY\n",
            "================================================\n",
            "[01] new_deaths         | True=       636 | Pred=       275\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "636InsensitiveBKICYICYICYICYICY\n",
            "================================================\n",
            "[02] new_deaths         | True=       636 | Pred=       275\n",
            "[03] new_deaths         | True=       636 | Pred=       275\n",
            "[04] new_deaths         | True=       636 | Pred=       275\n",
            "[05] new_deaths         | True=       636 | Pred=       275\n",
            "[06] new_deaths         | True=       636 | Pred=       297\n",
            "[07] new_deaths         | True=       805 | Pred=       297\n",
            "[08] new_deaths         | True=       805 | Pred=       297\n",
            "[09] new_deaths         | True=       805 | Pred=       297\n",
            "[10] new_deaths         | True=       805 | Pred=       297\n",
            "[11] new_deaths         | True=       805 | Pred=       297\n",
            "[12] new_deaths         | True=       805 | Pred=       297\n",
            "[13] new_deaths         | True=       805 | Pred=       418\n",
            "[14] new_deaths         | True=      1150 | Pred=       418\n",
            "[15] new_deaths         | True=      1150 | Pred=       418\n",
            "[16] new_deaths         | True=      1150 | Pred=       418\n",
            "[17] new_deaths         | True=      1150 | Pred=       418\n",
            "[18] new_deaths         | True=      1150 | Pred=       418\n",
            "[19] new_deaths         | True=      1150 | Pred=       418\n",
            "[20] new_deaths         | True=      1150 | Pred=       636\n",
            "[21] new_deaths         | True=      1454 | Pred=       636\n",
            "[22] new_deaths         | True=      1454 | Pred=       636\n",
            "[23] new_deaths         | True=      1454 | Pred=       636\n",
            "[24] new_deaths         | True=      1454 | Pred=       636\n",
            "[25] new_deaths         | True=      1454 | Pred=       636\n",
            "[26] new_deaths         | True=      1454 | Pred=       636\n",
            "[27] new_deaths         | True=      1454 | Pred=       805\n",
            "[28] new_deaths         | True=      1672 | Pred=       805\n",
            "[29] new_deaths         | True=      1672 | Pred=       805\n",
            "[30] new_deaths         | True=      1672 | Pred=       805\n",
            "\n",
            "============================================================\n",
            "📊 Base Llama-3 — new_deaths\n",
            "============================================================\n",
            "MAE:        621.93\n",
            "Scaled MSE: 0.043485\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating Base Model on 'people_vaccinated' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 186894730, 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "194ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[01] people_vaccinated  | True= 194530994 | Pred= 186894730\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270, 194530994\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "194ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[02] people_vaccinated  | True= 195086109 | Pred= 187193532\n",
            "[03] people_vaccinated  | True= 195649555 | Pred= 187540278\n",
            "[04] people_vaccinated  | True= 196279987 | Pred= 187764228\n",
            "[05] people_vaccinated  | True= 196665066 | Pred= 187899544\n",
            "[06] people_vaccinated  | True= 196904648 | Pred= 188238908\n",
            "[07] people_vaccinated  | True= 197428551 | Pred= 188609484\n",
            "[08] people_vaccinated  | True= 197962296 | Pred= 188996202\n",
            "[09] people_vaccinated  | True= 198464661 | Pred= 189414334\n",
            "[10] people_vaccinated  | True= 198982786 | Pred= 189900995\n",
            "[11] people_vaccinated  | True= 199567586 | Pred= 190202135\n",
            "[12] people_vaccinated  | True= 199932043 | Pred= 190404278\n",
            "[13] people_vaccinated  | True= 200144925 | Pred= 190851832\n",
            "[14] people_vaccinated  | True= 200611812 | Pred= 191333172\n",
            "[15] people_vaccinated  | True= 201104856 | Pred= 191825383\n",
            "[16] people_vaccinated  | True= 201594173 | Pred= 192324298\n",
            "[17] people_vaccinated  | True= 202076701 | Pred= 192902517\n",
            "[18] people_vaccinated  | True= 202617121 | Pred= 193250303\n",
            "[19] people_vaccinated  | True= 202940961 | Pred= 193481569\n",
            "[20] people_vaccinated  | True= 203127143 | Pred= 193990270\n",
            "[21] people_vaccinated  | True= 203569005 | Pred= 194530994\n",
            "[22] people_vaccinated  | True= 204031035 | Pred= 195086109\n",
            "[23] people_vaccinated  | True= 204500481 | Pred= 195649555\n",
            "[24] people_vaccinated  | True= 204976451 | Pred= 196279987\n",
            "[25] people_vaccinated  | True= 205509915 | Pred= 196665066\n",
            "[26] people_vaccinated  | True= 205829068 | Pred= 196904648\n",
            "[27] people_vaccinated  | True= 206022846 | Pred= 197428551\n",
            "[28] people_vaccinated  | True= 206442182 | Pred= 197962296\n",
            "[29] people_vaccinated  | True= 206865219 | Pred= 198464661\n",
            "[30] people_vaccinated  | True= 207292449 | Pred= 198982786\n",
            "\n",
            "============================================================\n",
            "📊 Base Llama-3 — people_vaccinated\n",
            "============================================================\n",
            "MAE:        8,857,932.67\n",
            "Scaled MSE: 0.001831\n",
            "============================================================\n",
            "\n",
            "===== SUMMARY — Base Llama-3 (Simple numeric output) =====\n",
            "new_cases            | MAE=60,464.25 | Scaled MSE=0.075356\n",
            "new_deaths           | MAE=621.93 | Scaled MSE=0.043485\n",
            "people_vaccinated    | MAE=8,857,932.67 | Scaled MSE=0.001831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import gc\n",
        "from contextlib import nullcontext\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ============================================================\n",
        "# 0. Sanity checks\n",
        "# ============================================================\n",
        "try:\n",
        "    model\n",
        "    tokenizer\n",
        "except NameError:\n",
        "    raise RuntimeError(\"'model' and 'tokenizer' must already be loaded (Llama-3).\")\n",
        "\n",
        "if 'df' not in globals():\n",
        "    raise RuntimeError(\"'df' not found. Load your timeseries first!\")\n",
        "\n",
        "TARGETS = ['new_cases', 'new_deaths', 'people_vaccinated']\n",
        "for col in TARGETS:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Column '{col}' not in df.columns\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. Autocast helper\n",
        "# ============================================================\n",
        "def autocast_ctx():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32)\n",
        "    return nullcontext()\n",
        "\n",
        "# ============================================================\n",
        "# 2. SIMPLE PREDICTOR (same as your base version)\n",
        "# ============================================================\n",
        "def get_llama_prediction(history_list, debug=False):\n",
        "    \"\"\"\n",
        "    Prompt Llama with a sequence of numbers and return the FIRST number\n",
        "    it generates after 'Next number:'.\n",
        "    \"\"\"\n",
        "    history_str = \", \".join(str(int(x)) for x in history_list)\n",
        "\n",
        "    prompt = (\n",
        "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "        \"You are a model that predicts the next value in a numerical time series.\\n\"\n",
        "        \"Given a sequence of numbers, output ONLY the next number.\\n\"\n",
        "        \"No words, no explanation, just the number.\\n\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "        f\"Sequence: {history_str}\\n\"\n",
        "        \"Next number:\\n\"\n",
        "        \"<|eot_id|>\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with autocast_ctx():\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=8,          # small, we only need one number\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if debug:\n",
        "        print(\"============== RAW MODEL RESPONSE ==============\")\n",
        "        print(response)\n",
        "        print(\"================================================\")\n",
        "\n",
        "    # FIRST number only\n",
        "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", response)\n",
        "    if m:\n",
        "        return float(m.group(0))\n",
        "\n",
        "    # if no number at all, fallback to last value\n",
        "    return float(history_list[-1])\n",
        "\n",
        "# ============================================================\n",
        "# 3. Evaluation function (simple numeric)\n",
        "# ============================================================\n",
        "def evaluate_target_simple(target_col, history_window=20, eval_last=30):\n",
        "    raw_series = df[target_col].values.astype(float)\n",
        "    split = int(len(raw_series) * 0.8)\n",
        "    test_series = raw_series[split:]\n",
        "\n",
        "    if len(test_series) <= history_window + 5:\n",
        "        raise ValueError(f\"Not enough test data for {target_col}\")\n",
        "\n",
        "    eval_len = min(eval_last, len(test_series) - history_window)\n",
        "    indices = range(len(test_series) - eval_len, len(test_series))\n",
        "\n",
        "    actuals, preds = [], []\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(raw_series.reshape(-1, 1))\n",
        "\n",
        "    print(f\"\\n🔍 Evaluating on '{target_col}' (last {len(indices)} pts)...\\n\")\n",
        "\n",
        "    for k, i in enumerate(indices, start=1):\n",
        "        hist = test_series[i - history_window : i]\n",
        "        actual = test_series[i]\n",
        "\n",
        "        pred = get_llama_prediction(hist, debug=(k <= 2))\n",
        "\n",
        "        actuals.append(actual)\n",
        "        preds.append(pred)\n",
        "\n",
        "        print(f\"[{k:02d}] {target_col:18s} | True={int(actual):10d} | Pred={int(pred):10d}\")\n",
        "\n",
        "    actuals = np.array(actuals, dtype=float)\n",
        "    preds = np.array(preds, dtype=float)\n",
        "\n",
        "    mae = mean_absolute_error(actuals, preds)\n",
        "\n",
        "    scaled_actuals = scaler.transform(actuals.reshape(-1, 1))\n",
        "    scaled_preds = scaler.transform(preds.reshape(-1, 1))\n",
        "    scaled_mse = mean_squared_error(scaled_actuals, scaled_preds)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"📊 {target_col}\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"MAE:        {mae:,.2f}\")\n",
        "    print(f\"Scaled MSE: {scaled_mse:.6f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return mae, scaled_mse\n",
        "\n",
        "# ============================================================\n",
        "# 4. BASELINE: evaluate BEFORE finetuning\n",
        "# ============================================================\n",
        "print(\"===== 🔹 BASE MODEL (before QLoRA) =====\")\n",
        "base_results = {}\n",
        "for col in TARGETS:\n",
        "    mae, mse = evaluate_target_simple(col)\n",
        "    base_results[col] = (mae, mse)\n",
        "\n",
        "print(\"\\n----- BASE SUMMARY -----\")\n",
        "for col in TARGETS:\n",
        "    mae, mse = base_results[col]\n",
        "    print(f\"{col:20s} | MAE={mae:,.2f} | Scaled MSE={mse:.6f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. BUILD QLoRA TRAIN DATASET (all 3 targets, same prompt style)\n",
        "# ============================================================\n",
        "print(\"\\n===== 🧪 Building QLoRA training dataset (all 3 targets) =====\")\n",
        "\n",
        "history_window = 20\n",
        "train_samples = []\n",
        "\n",
        "for target_col in TARGETS:\n",
        "    series = df[target_col].values.astype(float)\n",
        "    # use training portion only (same 80% split)\n",
        "    split = int(len(series) * 0.8)\n",
        "    train_series = series[:split]\n",
        "\n",
        "    for i in range(history_window, len(train_series)):\n",
        "        hist = train_series[i - history_window:i]\n",
        "        target = train_series[i]\n",
        "\n",
        "        hist_str = \", \".join(str(int(x)) for x in hist)\n",
        "\n",
        "        text = (\n",
        "            \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
        "            \"You are a model that predicts the next value in a numerical time series.\\n\"\n",
        "            \"Given a sequence of numbers, output ONLY the next number.\\n\"\n",
        "            \"No words, no explanation, just the number.\\n\"\n",
        "            \"<|eot_id|>\"\n",
        "            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            f\"Sequence: {hist_str}\\n\"\n",
        "            \"Next number:\\n\"\n",
        "            \"<|eot_id|>\"\n",
        "            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "            f\"{int(target)}\"\n",
        "        )\n",
        "\n",
        "        train_samples.append({\"text\": text})\n",
        "\n",
        "train_ds = Dataset.from_list(train_samples)\n",
        "print(f\"✅ QLoRA train samples: {len(train_ds)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. QLoRA CONFIG + SHORT FINETUNE\n",
        "# ============================================================\n",
        "print(\"\\n===== 🔧 Starting QLoRA finetune (short run) =====\")\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./llama_covid_timeseries_qlora_simple\",\n",
        "    dataset_text_field=\"text\",\n",
        "    max_steps=60,                      # short run\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    peft_config=peft_config,\n",
        "    args=sft_config,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"✅ QLoRA finetune complete.\")\n",
        "\n",
        "# ============================================================\n",
        "# 7. EVALUATE AFTER FINETUNE (same pipeline)\n",
        "# ============================================================\n",
        "print(\"\\n===== 🔹 MODEL AFTER QLoRA FINETUNE =====\")\n",
        "ft_results = {}\n",
        "for col in TARGETS:\n",
        "    mae, mse = evaluate_target_simple(col)\n",
        "    ft_results[col] = (mae, mse)\n",
        "\n",
        "print(\"\\n===== 📊 BASE vs QLoRA SUMMARY =====\")\n",
        "for col in TARGETS:\n",
        "    b_mae, b_mse = base_results[col]\n",
        "    f_mae, f_mse = ft_results[col]\n",
        "    print(f\"{col:20s}\")\n",
        "    print(f\"  Base  MAE={b_mae:,.2f} | Scaled MSE={b_mse:.6f}\")\n",
        "    print(f\"  QLoRA MAE={f_mae:,.2f} | Scaled MSE={f_mse:.6f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "813773631aee4268b2a84f2779027215",
            "1d0a7db36a6c404caf5f5a6e725f59b2",
            "d6429d3272474a2a818724df10303915",
            "45b09a17444843e091d79a63c7b377a6",
            "9b5dd39b5ddb4ea9a969fd2b25003d0a",
            "cdf21fc71a8543b7a5f1b8141cdd47b2",
            "aabfe371542e4b108fa1ea35a3e5e364",
            "2e338661aab94ec194051ddfa804fff4",
            "f10bf1efd14a43ec8d79a65257ac5ee5",
            "47a98d93e0a648e89fd654207d5e7a81",
            "f8de0563cd624e64b048f08e8fc2d9f0",
            "cdc4d8e9f3e54e6684451201d9fd2f46",
            "0756b8daf6d74efa8a34e8ed56c5dfb0",
            "43fee8309f9b407baa62cd16b586b0b4",
            "006bbd7ee95042cc9ba4db5106dc6f52",
            "b1ca65c9977b4adb9b11aa23f9220663",
            "3285a0dca1fd42328807a5196693c0af",
            "c3f050dc4acb420dbb643a749b346e83",
            "5d1da5144d4e4d8d9a2882a627984f9d",
            "5d98bc7157244a16949a4d762f15d7ab",
            "9ca4dc714b7a481c9da7776f5a818cdb",
            "e08a87400c134df285be7e81ad940009",
            "4d67b80a68f34824a82d339dbddfd56f",
            "4274f5ed71cc485e8af7736fa9d95008",
            "d83228294c6e4d24838556de35dac56d",
            "b225cc77f2944d5993eae1088eaade3d",
            "82c41dc3aeec4828a61845c27d7c29c7",
            "d09f7e219740444b80fee6caa0a862bd",
            "81029db8489047a38362bf476e61cae7",
            "0740d93c0dc74bdd90255c66c31370bb",
            "55493bdf47b046b4bbd66fb7eb72a82a",
            "282338a1924b44e7a40062ddd3f4c465",
            "3fa6e5689570495f805ff703816ee19c"
          ]
        },
        "id": "Sxk_LMocHwgi",
        "outputId": "aeaddb7c-3900-44fa-d0ec-d284f954729e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== 🔹 BASE MODEL (before QLoRA) =====\n",
            "\n",
            "🔍 Evaluating on 'new_cases' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "106ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[01] new_cases          | True=    106150 | Pred=     31797\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "106ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[02] new_cases          | True=    106150 | Pred=     31797\n",
            "[03] new_cases          | True=    106150 | Pred=     31797\n",
            "[04] new_cases          | True=    106150 | Pred=     31797\n",
            "[05] new_cases          | True=    106150 | Pred=     31797\n",
            "[06] new_cases          | True=    106150 | Pred=     51289\n",
            "[07] new_cases          | True=    126637 | Pred=     51289\n",
            "[08] new_cases          | True=    126637 | Pred=     51289\n",
            "[09] new_cases          | True=    126637 | Pred=     51289\n",
            "[10] new_cases          | True=    126637 | Pred=     51289\n",
            "[11] new_cases          | True=    126637 | Pred=     51289\n",
            "[12] new_cases          | True=    126637 | Pred=     51289\n",
            "[13] new_cases          | True=    126637 | Pred=     78155\n",
            "[14] new_cases          | True=    145709 | Pred=     78155\n",
            "[15] new_cases          | True=    145709 | Pred=     78155\n",
            "[16] new_cases          | True=    145709 | Pred=     78155\n",
            "[17] new_cases          | True=    145709 | Pred=     78155\n",
            "[18] new_cases          | True=    145709 | Pred=     78155\n",
            "[19] new_cases          | True=    145709 | Pred=     78155\n",
            "[20] new_cases          | True=    145709 | Pred=    106150\n",
            "[21] new_cases          | True=    156412 | Pred=    106150\n",
            "[22] new_cases          | True=    156412 | Pred=    106150\n",
            "[23] new_cases          | True=    156412 | Pred=    106150\n",
            "[24] new_cases          | True=    156412 | Pred=    106150\n",
            "[25] new_cases          | True=    156412 | Pred=    106150\n",
            "[26] new_cases          | True=    156412 | Pred=    106150\n",
            "[27] new_cases          | True=    156412 | Pred=    126637\n",
            "[28] new_cases          | True=    163464 | Pred=    126637\n",
            "[29] new_cases          | True=    163464 | Pred=    126637\n",
            "[30] new_cases          | True=    163464 | Pred=    126637\n",
            "\n",
            "============================================================\n",
            "📊 new_cases\n",
            "============================================================\n",
            "MAE:        60,464.25\n",
            "Scaled MSE: 0.075356\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating on 'new_deaths' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "636InsensitiveBKInsensitiveBKInsensitiveBKInsensitive\n",
            "================================================\n",
            "[01] new_deaths         | True=       636 | Pred=       275\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "636InsensitiveBKInsensitiveBKInsensitiveBKICY\n",
            "================================================\n",
            "[02] new_deaths         | True=       636 | Pred=       275\n",
            "[03] new_deaths         | True=       636 | Pred=       275\n",
            "[04] new_deaths         | True=       636 | Pred=       275\n",
            "[05] new_deaths         | True=       636 | Pred=       275\n",
            "[06] new_deaths         | True=       636 | Pred=       297\n",
            "[07] new_deaths         | True=       805 | Pred=       297\n",
            "[08] new_deaths         | True=       805 | Pred=       297\n",
            "[09] new_deaths         | True=       805 | Pred=       297\n",
            "[10] new_deaths         | True=       805 | Pred=       297\n",
            "[11] new_deaths         | True=       805 | Pred=       297\n",
            "[12] new_deaths         | True=       805 | Pred=       297\n",
            "[13] new_deaths         | True=       805 | Pred=       418\n",
            "[14] new_deaths         | True=      1150 | Pred=       418\n",
            "[15] new_deaths         | True=      1150 | Pred=       418\n",
            "[16] new_deaths         | True=      1150 | Pred=       418\n",
            "[17] new_deaths         | True=      1150 | Pred=       418\n",
            "[18] new_deaths         | True=      1150 | Pred=       418\n",
            "[19] new_deaths         | True=      1150 | Pred=       418\n",
            "[20] new_deaths         | True=      1150 | Pred=       636\n",
            "[21] new_deaths         | True=      1454 | Pred=       636\n",
            "[22] new_deaths         | True=      1454 | Pred=       636\n",
            "[23] new_deaths         | True=      1454 | Pred=       636\n",
            "[24] new_deaths         | True=      1454 | Pred=       636\n",
            "[25] new_deaths         | True=      1454 | Pred=       636\n",
            "[26] new_deaths         | True=      1454 | Pred=       636\n",
            "[27] new_deaths         | True=      1454 | Pred=       805\n",
            "[28] new_deaths         | True=      1672 | Pred=       805\n",
            "[29] new_deaths         | True=      1672 | Pred=       805\n",
            "[30] new_deaths         | True=      1672 | Pred=       805\n",
            "\n",
            "============================================================\n",
            "📊 new_deaths\n",
            "============================================================\n",
            "MAE:        621.93\n",
            "Scaled MSE: 0.043485\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating on 'people_vaccinated' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 186894730, 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "194ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[01] people_vaccinated  | True= 194530994 | Pred= 186894730\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270, 194530994\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "194ICYICYICYICYICYICYICY\n",
            "================================================\n",
            "[02] people_vaccinated  | True= 195086109 | Pred= 187193532\n",
            "[03] people_vaccinated  | True= 195649555 | Pred= 187540278\n",
            "[04] people_vaccinated  | True= 196279987 | Pred= 187764228\n",
            "[05] people_vaccinated  | True= 196665066 | Pred= 187899544\n",
            "[06] people_vaccinated  | True= 196904648 | Pred= 188238908\n",
            "[07] people_vaccinated  | True= 197428551 | Pred= 188609484\n",
            "[08] people_vaccinated  | True= 197962296 | Pred= 188996202\n",
            "[09] people_vaccinated  | True= 198464661 | Pred= 189414334\n",
            "[10] people_vaccinated  | True= 198982786 | Pred= 189900995\n",
            "[11] people_vaccinated  | True= 199567586 | Pred= 190202135\n",
            "[12] people_vaccinated  | True= 199932043 | Pred= 190404278\n",
            "[13] people_vaccinated  | True= 200144925 | Pred= 190851832\n",
            "[14] people_vaccinated  | True= 200611812 | Pred= 191333172\n",
            "[15] people_vaccinated  | True= 201104856 | Pred= 191825383\n",
            "[16] people_vaccinated  | True= 201594173 | Pred= 192324298\n",
            "[17] people_vaccinated  | True= 202076701 | Pred= 192902517\n",
            "[18] people_vaccinated  | True= 202617121 | Pred= 193250303\n",
            "[19] people_vaccinated  | True= 202940961 | Pred= 193481569\n",
            "[20] people_vaccinated  | True= 203127143 | Pred= 193990270\n",
            "[21] people_vaccinated  | True= 203569005 | Pred= 194530994\n",
            "[22] people_vaccinated  | True= 204031035 | Pred= 195086109\n",
            "[23] people_vaccinated  | True= 204500481 | Pred= 195649555\n",
            "[24] people_vaccinated  | True= 204976451 | Pred= 196279987\n",
            "[25] people_vaccinated  | True= 205509915 | Pred= 196665066\n",
            "[26] people_vaccinated  | True= 205829068 | Pred= 196904648\n",
            "[27] people_vaccinated  | True= 206022846 | Pred= 197428551\n",
            "[28] people_vaccinated  | True= 206442182 | Pred= 197962296\n",
            "[29] people_vaccinated  | True= 206865219 | Pred= 198464661\n",
            "[30] people_vaccinated  | True= 207292449 | Pred= 198982786\n",
            "\n",
            "============================================================\n",
            "📊 people_vaccinated\n",
            "============================================================\n",
            "MAE:        8,857,932.67\n",
            "Scaled MSE: 0.001831\n",
            "============================================================\n",
            "\n",
            "----- BASE SUMMARY -----\n",
            "new_cases            | MAE=60,464.25 | Scaled MSE=0.075356\n",
            "new_deaths           | MAE=621.93 | Scaled MSE=0.043485\n",
            "people_vaccinated    | MAE=8,857,932.67 | Scaled MSE=0.001831\n",
            "\n",
            "===== 🧪 Building QLoRA training dataset (all 3 targets) =====\n",
            "✅ QLoRA train samples: 570\n",
            "\n",
            "===== 🔧 Starting QLoRA finetune (short run) =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "813773631aee4268b2a84f2779027215"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdc4d8e9f3e54e6684451201d9fd2f46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d67b80a68f34824a82d339dbddfd56f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 02:50, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.492700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.348200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.044700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.038900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.157700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.186900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QLoRA finetune complete.\n",
            "\n",
            "===== 🔹 MODEL AFTER QLoRA FINETUNE =====\n",
            "\n",
            "🔍 Evaluating on 'new_cases' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "106Insensitiveisay​\n",
            "\n",
            "def.GraphicsUnit.GraphicsUnit.GraphicsUnit\n",
            "================================================\n",
            "[01] new_cases          | True=    106150 | Pred=     31797\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 31797, 31797, 31797, 31797, 51289, 51289, 51289, 51289, 51289, 51289, 51289, 78155, 78155, 78155, 78155, 78155, 78155, 78155, 106150, 106150\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "106Insensitiveisay​\n",
            "\n",
            "def.GraphicsUnit.GraphicsUnit.GraphicsUnit\n",
            "================================================\n",
            "[02] new_cases          | True=    106150 | Pred=     31797\n",
            "[03] new_cases          | True=    106150 | Pred=     31797\n",
            "[04] new_cases          | True=    106150 | Pred=     31797\n",
            "[05] new_cases          | True=    106150 | Pred=     31797\n",
            "[06] new_cases          | True=    106150 | Pred=     51289\n",
            "[07] new_cases          | True=    126637 | Pred=     51289\n",
            "[08] new_cases          | True=    126637 | Pred=     51289\n",
            "[09] new_cases          | True=    126637 | Pred=     51289\n",
            "[10] new_cases          | True=    126637 | Pred=     51289\n",
            "[11] new_cases          | True=    126637 | Pred=     51289\n",
            "[12] new_cases          | True=    126637 | Pred=     51289\n",
            "[13] new_cases          | True=    126637 | Pred=     78155\n",
            "[14] new_cases          | True=    145709 | Pred=     78155\n",
            "[15] new_cases          | True=    145709 | Pred=     78155\n",
            "[16] new_cases          | True=    145709 | Pred=     78155\n",
            "[17] new_cases          | True=    145709 | Pred=     78155\n",
            "[18] new_cases          | True=    145709 | Pred=     78155\n",
            "[19] new_cases          | True=    145709 | Pred=     78155\n",
            "[20] new_cases          | True=    145709 | Pred=    106150\n",
            "[21] new_cases          | True=    156412 | Pred=    106150\n",
            "[22] new_cases          | True=    156412 | Pred=    106150\n",
            "[23] new_cases          | True=    156412 | Pred=    106150\n",
            "[24] new_cases          | True=    156412 | Pred=    106150\n",
            "[25] new_cases          | True=    156412 | Pred=    106150\n",
            "[26] new_cases          | True=    156412 | Pred=    106150\n",
            "[27] new_cases          | True=    156412 | Pred=    126637\n",
            "[28] new_cases          | True=    163464 | Pred=    126637\n",
            "[29] new_cases          | True=    163464 | Pred=    126637\n",
            "[30] new_cases          | True=    163464 | Pred=    126637\n",
            "\n",
            "============================================================\n",
            "📊 new_cases\n",
            "============================================================\n",
            "MAE:        60,464.25\n",
            "Scaled MSE: 0.075356\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating on 'new_deaths' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "636Insensitiveisay​\n",
            "\n",
            "def.GraphicsUnit.GraphicsUnit.GraphicsUnit\n",
            "================================================\n",
            "[01] new_deaths         | True=       636 | Pred=       275\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 275, 275, 275, 275, 297, 297, 297, 297, 297, 297, 297, 418, 418, 418, 418, 418, 418, 418, 636, 636\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "636Insensitiveisay​\n",
            "\n",
            "def.GraphicsUnit.GraphicsUnit.GraphicsUnit\n",
            "================================================\n",
            "[02] new_deaths         | True=       636 | Pred=       275\n",
            "[03] new_deaths         | True=       636 | Pred=       275\n",
            "[04] new_deaths         | True=       636 | Pred=       275\n",
            "[05] new_deaths         | True=       636 | Pred=       275\n",
            "[06] new_deaths         | True=       636 | Pred=       297\n",
            "[07] new_deaths         | True=       805 | Pred=       297\n",
            "[08] new_deaths         | True=       805 | Pred=       297\n",
            "[09] new_deaths         | True=       805 | Pred=       297\n",
            "[10] new_deaths         | True=       805 | Pred=       297\n",
            "[11] new_deaths         | True=       805 | Pred=       297\n",
            "[12] new_deaths         | True=       805 | Pred=       297\n",
            "[13] new_deaths         | True=       805 | Pred=       418\n",
            "[14] new_deaths         | True=      1150 | Pred=       418\n",
            "[15] new_deaths         | True=      1150 | Pred=       418\n",
            "[16] new_deaths         | True=      1150 | Pred=       418\n",
            "[17] new_deaths         | True=      1150 | Pred=       418\n",
            "[18] new_deaths         | True=      1150 | Pred=       418\n",
            "[19] new_deaths         | True=      1150 | Pred=       418\n",
            "[20] new_deaths         | True=      1150 | Pred=       636\n",
            "[21] new_deaths         | True=      1454 | Pred=       636\n",
            "[22] new_deaths         | True=      1454 | Pred=       636\n",
            "[23] new_deaths         | True=      1454 | Pred=       636\n",
            "[24] new_deaths         | True=      1454 | Pred=       636\n",
            "[25] new_deaths         | True=      1454 | Pred=       636\n",
            "[26] new_deaths         | True=      1454 | Pred=       636\n",
            "[27] new_deaths         | True=      1454 | Pred=       805\n",
            "[28] new_deaths         | True=      1672 | Pred=       805\n",
            "[29] new_deaths         | True=      1672 | Pred=       805\n",
            "[30] new_deaths         | True=      1672 | Pred=       805\n",
            "\n",
            "============================================================\n",
            "📊 new_deaths\n",
            "============================================================\n",
            "MAE:        621.93\n",
            "Scaled MSE: 0.043485\n",
            "============================================================\n",
            "\n",
            "🔍 Evaluating on 'people_vaccinated' (last 30 pts)...\n",
            "\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 186894730, 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "194ICYInsensitiveisay​\n",
            "\n",
            "def.GraphicsUnit.GraphicsUnit\n",
            "================================================\n",
            "[01] people_vaccinated  | True= 194530994 | Pred= 186894730\n",
            "============== RAW MODEL RESPONSE ==============\n",
            "system\n",
            "\n",
            "You are a model that predicts the next value in a numerical time series.\n",
            "Given a sequence of numbers, output ONLY the next number.\n",
            "No words, no explanation, just the number.\n",
            "user\n",
            "\n",
            "Sequence: 187193532, 187540278, 187764228, 187899544, 188238908, 188609484, 188996202, 189414334, 189900995, 190202135, 190404278, 190851832, 191333172, 191825383, 192324298, 192902517, 193250303, 193481569, 193990270, 194530994\n",
            "Next number:\n",
            "assistant\n",
            "\n",
            "195ICYInsensitiveisay​\n",
            "\n",
            "def.GraphicsUnit.GraphicsUnit\n",
            "================================================\n",
            "[02] people_vaccinated  | True= 195086109 | Pred= 187193532\n",
            "[03] people_vaccinated  | True= 195649555 | Pred= 187540278\n",
            "[04] people_vaccinated  | True= 196279987 | Pred= 187764228\n",
            "[05] people_vaccinated  | True= 196665066 | Pred= 187899544\n",
            "[06] people_vaccinated  | True= 196904648 | Pred= 188238908\n",
            "[07] people_vaccinated  | True= 197428551 | Pred= 188609484\n",
            "[08] people_vaccinated  | True= 197962296 | Pred= 188996202\n",
            "[09] people_vaccinated  | True= 198464661 | Pred= 189414334\n",
            "[10] people_vaccinated  | True= 198982786 | Pred= 189900995\n",
            "[11] people_vaccinated  | True= 199567586 | Pred= 190202135\n",
            "[12] people_vaccinated  | True= 199932043 | Pred= 190404278\n",
            "[13] people_vaccinated  | True= 200144925 | Pred= 190851832\n",
            "[14] people_vaccinated  | True= 200611812 | Pred= 191333172\n",
            "[15] people_vaccinated  | True= 201104856 | Pred= 191825383\n",
            "[16] people_vaccinated  | True= 201594173 | Pred= 192324298\n",
            "[17] people_vaccinated  | True= 202076701 | Pred= 192902517\n",
            "[18] people_vaccinated  | True= 202617121 | Pred= 193250303\n",
            "[19] people_vaccinated  | True= 202940961 | Pred= 193481569\n",
            "[20] people_vaccinated  | True= 203127143 | Pred= 193990270\n",
            "[21] people_vaccinated  | True= 203569005 | Pred= 194530994\n",
            "[22] people_vaccinated  | True= 204031035 | Pred= 195086109\n",
            "[23] people_vaccinated  | True= 204500481 | Pred= 195649555\n",
            "[24] people_vaccinated  | True= 204976451 | Pred= 196279987\n",
            "[25] people_vaccinated  | True= 205509915 | Pred= 196665066\n",
            "[26] people_vaccinated  | True= 205829068 | Pred= 196904648\n",
            "[27] people_vaccinated  | True= 206022846 | Pred= 197428551\n",
            "[28] people_vaccinated  | True= 206442182 | Pred= 197962296\n",
            "[29] people_vaccinated  | True= 206865219 | Pred= 198464661\n",
            "[30] people_vaccinated  | True= 207292449 | Pred= 198982786\n",
            "\n",
            "============================================================\n",
            "📊 people_vaccinated\n",
            "============================================================\n",
            "MAE:        8,857,932.67\n",
            "Scaled MSE: 0.001831\n",
            "============================================================\n",
            "\n",
            "===== 📊 BASE vs QLoRA SUMMARY =====\n",
            "new_cases           \n",
            "  Base  MAE=60,464.25 | Scaled MSE=0.075356\n",
            "  QLoRA MAE=60,464.25 | Scaled MSE=0.075356\n",
            "--------------------------------------------------\n",
            "new_deaths          \n",
            "  Base  MAE=621.93 | Scaled MSE=0.043485\n",
            "  QLoRA MAE=621.93 | Scaled MSE=0.043485\n",
            "--------------------------------------------------\n",
            "people_vaccinated   \n",
            "  Base  MAE=8,857,932.67 | Scaled MSE=0.001831\n",
            "  QLoRA MAE=8,857,932.67 | Scaled MSE=0.001831\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
